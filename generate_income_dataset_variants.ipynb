{"cells":[{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# coding=utf-8\n","#\n","# The copyright of this file belongs to Feedzai. The file cannot be\n","# reproduced in whole or in part, stored in a retrieval system,\n","# transmitted in any form, or by any means electronic, mechanical,\n","# photocopying, or otherwise, without the prior permission of the owner.\n","#\n","# (c) 2022 Feedzai, Strictly Confidential\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"mYF03vl4xvbg","outputId":"526111ce-46df-43f0-edb6-3e430d845d7b","executionInfo":{"status":"ok","timestamp":1690390318299,"user_tz":300,"elapsed":16498,"user":{"displayName":"Erik Johansson","userId":"01074076572453530700"}}},"id":"mYF03vl4xvbg"},{"cell_type":"code","execution_count":null,"outputs":[],"source":["import numpy as np   # Seed generation\n","import pandas as pd  # Matrix operations"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"yoHu30F3xvbk"},"id":"yoHu30F3xvbk"},{"cell_type":"code","execution_count":null,"id":"9b512d99","metadata":{"pycharm":{"name":"#%%\n"},"id":"9b512d99"},"outputs":[],"source":["# Reading the 3M sample:\n","large_sample_path = \"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Synthetic Data/synthetic-data-merged.csv\"\n","large_sample_df = pd.read_csv(large_sample_path)\n","\n","# Reading the original (with same preprocessed features) dataset:\n","original_sample_path = \"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Base.csv\"\n","original_sample_df = pd.read_csv(original_sample_path)"]},{"cell_type":"code","source":["print(large_sample_df.columns)\n","large_sample_df = large_sample_df.drop(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0'], axis=1) # drop additional columns created through merging datasets\n","print(large_sample_df.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9S6Zz0wyASS","executionInfo":{"status":"ok","timestamp":1690390357017,"user_tz":300,"elapsed":289,"user":{"displayName":"Erik Johansson","userId":"01074076572453530700"}},"outputId":"f66d6172-7e51-492f-9c96-58c05f7f4460"},"id":"E9S6Zz0wyASS","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'fraud_bool', 'income',\n","       'name_email_similarity', 'prev_address_months_count',\n","       'current_address_months_count', 'customer_age', 'days_since_request',\n","       'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h',\n","       'velocity_24h', 'velocity_4w', 'bank_branch_count_8w',\n","       'date_of_birth_distinct_emails_4w', 'employment_status',\n","       'credit_risk_score', 'email_is_free', 'housing_status',\n","       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n","       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n","       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n","       'device_distinct_emails_8w', 'device_fraud_count', 'month'],\n","      dtype='object')\n","Index(['fraud_bool', 'income', 'name_email_similarity',\n","       'prev_address_months_count', 'current_address_months_count',\n","       'customer_age', 'days_since_request', 'intended_balcon_amount',\n","       'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\n","       'velocity_4w', 'bank_branch_count_8w',\n","       'date_of_birth_distinct_emails_4w', 'employment_status',\n","       'credit_risk_score', 'email_is_free', 'housing_status',\n","       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n","       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n","       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n","       'device_distinct_emails_8w', 'device_fraud_count', 'month'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["large_sample_df.head() # check df"],"metadata":{"id":"WPjZQnTqyzd3"},"id":"WPjZQnTqyzd3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_sample_df.head() # check df"],"metadata":{"id":"K3sZrBPcy6ul"},"id":"K3sZrBPcy6ul","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking minimum and maximum income values within large and base dataframes\n","\n","print(f\"Highest income for large_sample_df: {large_sample_df['income'].max()}\")\n","print(f\"Lowest income for large_sample_df: {large_sample_df['income'].min()}\\n\")\n","\n","print(f\"Highest income for original_sample_df: {original_sample_df['income'].max()}\")\n","print(f\"Lowest income for original_sample_df: {original_sample_df['income'].min()}\\n\")"],"metadata":{"id":"m49A1KYqDUug","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690390422304,"user_tz":300,"elapsed":75,"user":{"displayName":"Erik Johansson","userId":"01074076572453530700"}},"outputId":"96f91ee3-a1bc-4049-e7fd-a910ffba7a03"},"id":"m49A1KYqDUug","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Highest income for large_sample_df: 0.9\n","Lowest income for large_sample_df: 0.1\n","\n","Highest income for original_sample_df: 0.9\n","Lowest income for original_sample_df: 0.1\n","\n"]}]},{"cell_type":"code","source":["exact_70_income = large_sample_df.loc[(large_sample_df['income'] == 0.7)] # check if any samples have income exactly equal to split value\n","print(len(exact_70_income))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiGqwCQHwx09","executionInfo":{"status":"ok","timestamp":1690390425150,"user_tz":300,"elapsed":1247,"user":{"displayName":"Erik Johansson","userId":"01074076572453530700"}},"outputId":"11b4c2c0-32ee-4499-b91c-3e09ea34f946"},"id":"YiGqwCQHwx09","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","execution_count":null,"id":"724feace","metadata":{"pycharm":{"name":"#%%\n"},"id":"724feace"},"outputs":[],"source":["# Obtain month frequency and fraud prevalence per month (on original data).\n","month_frequency =  original_sample_df[\"month\"].value_counts(normalize=True).to_dict()\n","month_fraud_prev = original_sample_df.groupby(\"month\")[\"fraud_bool\"].mean().to_dict()\n","# We cast to dict in order to facilitate the next operations."]},{"cell_type":"code","execution_count":null,"id":"51be6b2c","metadata":{"pycharm":{"name":"#%%\n"},"id":"51be6b2c"},"outputs":[],"source":["# Calculating the expected number of positive and negative instances,\n","# per month, given the observed month frequency and prevalence.\n","\n","sample_size = 1e6\n","\n","expected_positives = {}\n","expected_negatives = {}\n","\n","for month in month_fraud_prev.keys():\n","    expected_positives[month] = round(sample_size * month_frequency[month] * month_fraud_prev[month])\n","    expected_negatives[month] = round(sample_size * month_frequency[month] * (1-month_fraud_prev[month]))"]},{"cell_type":"code","execution_count":null,"id":"e62035e5","metadata":{"pycharm":{"name":"#%%\n"},"id":"e62035e5"},"outputs":[],"source":["# Sampling the \"Base\" dataset: Same month frequency and fraud rate per month.\n","base_dfs = []\n","\n","SEED = 42\n","\n","num_months = len(large_sample_df[\"month\"].unique())\n","seed_possible_values = list(range(1_000_000))\n","seed_list = np.random.choice(seed_possible_values, size=num_months, replace=False)\n","\n","for month, seed in zip(large_sample_df[\"month\"].unique(), seed_list):\n","    positive_pool = large_sample_df[(large_sample_df[\"month\"]==month) & (large_sample_df[\"fraud_bool\"]==1)]\n","    negative_pool = large_sample_df[(large_sample_df[\"month\"]==month) & (large_sample_df[\"fraud_bool\"]==0)]\n","\n","    positive_sample = positive_pool.sample(expected_positives[month], random_state=seed)\n","    negative_sample = negative_pool.sample(expected_negatives[month], random_state=seed+SEED)\n","\n","    base_dfs.extend([positive_sample, negative_sample])"]},{"cell_type":"code","execution_count":null,"id":"e980b2b1","metadata":{"pycharm":{"name":"#%%\n"},"id":"e980b2b1"},"outputs":[],"source":["# Concatenate the filtered samples to obtain the final dataset.\n","base_df = pd.concat(base_dfs)"]},{"cell_type":"code","execution_count":null,"id":"b23c18ab","metadata":{"pycharm":{"name":"#%%\n"},"id":"b23c18ab"},"outputs":[],"source":["# Now generating the biased samples.\n","# We will start by defining the protected groups.\n","large_sample_df[\"group\"] = (large_sample_df[\"income\"] < 0.7).map({True:\"Minority\", False: \"Majority\"})"]},{"cell_type":"code","execution_count":null,"id":"9ad941c9","metadata":{"pycharm":{"name":"#%%\n"},"id":"9ad941c9"},"outputs":[],"source":["# Helper method to define the joint probability of each combination of\n","# group and label.\n","\n","def calculate_probabilities(\n","    original_prevalence: float,\n","    prev_ratio: float,\n","    maj_pct: float,\n","):\n","    # Probability notation (p_maj = P(A=maj))\n","    p_maj = maj_pct\n","    p_min = 1 - p_maj\n","\n","    # Calculate prevalence for each class\n","    prev_min = original_prevalence / (prev_ratio * p_maj + (1 - p_maj))\n","    prev_maj = prev_ratio * prev_min\n","\n","    # Calculate joint and conditional probabilities of majority group\n","    p_maj_and_pos = prev_maj * p_maj\n","    p_maj_giv_pos: float = p_maj_and_pos / original_prevalence\n","    p_maj_and_neg = p_maj - p_maj_and_pos\n","    p_maj_giv_neg: float = p_maj_and_neg / (1 - original_prevalence)\n","\n","    # Calculate joint and conditional probabilities of minority group\n","    p_min_and_pos = prev_min * p_min\n","    p_min_giv_pos: float = p_min_and_pos / original_prevalence\n","    p_min_and_neg = p_min - p_min_and_pos\n","    p_min_giv_neg: float = p_min_and_neg / (1 - original_prevalence)\n","\n","    return p_min_and_pos, p_maj_and_pos, p_min_and_neg, p_maj_and_neg"]},{"cell_type":"code","execution_count":null,"id":"fd064658","metadata":{"pycharm":{"name":"#%%\n"},"id":"fd064658"},"outputs":[],"source":["# Helper method to obtain a dataframe from given group, month and label.\n","def get_filtered_df(large_sample_df, group, month, label):\n","    return large_sample_df[\n","        (large_sample_df[\"month\"]==month) &\n","        (large_sample_df[\"group\"]==group) &\n","        (large_sample_df[\"fraud_bool\"]==label)]\n","\n","\n","# Method to generate a biased sample controling group size or prevalence (fraud rate)\n","def group_prevalence_disparity(large_sample_df, original_sample_df, majority_size, fraud_rate_disparity):\n","    seed_list = np.random.choice(seed_possible_values, size=num_months, replace=False)\n","\n","    bias_dfs = []\n","\n","    # Allow for different majority sizes/fraud rates depending on the month of data.\n","    # This replicates a value if only one is passed.\n","    if isinstance(majority_size, float):\n","        majority_size=[majority_size]*original_sample_df[\"month\"].unique().shape[0]\n","    if isinstance(fraud_rate_disparity, (int, float)):\n","        fraud_rate_disparity=[fraud_rate_disparity]*original_sample_df[\"month\"].unique().shape[0]\n","\n","    for month, seed, maj_size, fr_disp in zip(large_sample_df[\"month\"].unique(), seed_list, majority_size, fraud_rate_disparity):\n","        month_prevalence = original_sample_df[original_sample_df[\"month\"]==month][\"fraud_bool\"].mean()\n","        (\n","            p_min_and_pos,\n","            p_maj_and_pos,\n","            p_min_and_neg,\n","            p_maj_and_neg,\n","        ) = calculate_probabilities(month_prevalence, 1/fr_disp, maj_size)\n","\n","        month_size = original_sample_df[\"month\"].value_counts(normalize=True)[month]*sample_size\n","\n","        # Calculate the needed amount of each combination of group/label to satisfy the disparities in month.\n","        n_minority_positive = round(month_size*p_min_and_pos, 0)\n","        n_minority_negative = round(month_size*p_min_and_neg, 0)\n","        n_majority_positive = round(month_size*p_maj_and_pos, 0)\n","        n_majority_negative = round(month_size*p_maj_and_neg, 0)\n","\n","        # Sample the large sample with expected values.\n","        bias_dfs.extend(\n","        [\n","            get_filtered_df(large_sample_df, \"Minority\", month, 1).sample(int(n_minority_positive), random_state=seed),\n","            get_filtered_df(large_sample_df, \"Minority\", month, 0).sample(int(n_minority_negative), random_state=seed+SEED),\n","            get_filtered_df(large_sample_df, \"Majority\", month, 1).sample(int(n_majority_positive), random_state=seed+2*SEED),\n","            get_filtered_df(large_sample_df, \"Majority\", month, 0).sample(int(n_majority_negative), random_state=seed+3*SEED),\n","        ]\n","        )\n","\n","    return pd.concat(bias_dfs)"]},{"cell_type":"code","execution_count":null,"id":"ce5f6ad4","metadata":{"pycharm":{"name":"#%%\n"},"id":"ce5f6ad4"},"outputs":[],"source":["# Params for the generated sample\n","majority_size = 0.9      # Relative size of the majority group\n","fraud_rate_disparity = 1 # fraud prevalence in minority / fraud prevalence in majority\n","\n","# For Type I we want to test group size disparity.\n","# Majority will have 90% of instances, Minority 10% of instances."]},{"cell_type":"code","execution_count":null,"id":"07204aad","metadata":{"pycharm":{"name":"#%%\n"},"id":"07204aad"},"outputs":[],"source":["typeI_df = group_prevalence_disparity(large_sample_df, original_sample_df, majority_size, fraud_rate_disparity)\n","typeI_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Income Data Variants 1m/income_07_type1.csv\")"]},{"cell_type":"code","source":["majority_size = 0.9\n","fraud_rate_disparity = 3\n","typeII_df = group_prevalence_disparity(large_sample_df,original_sample_df,majority_size,fraud_rate_disparity)\n","typeII_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Income Data Variants 1m/income_07_type2.csv\")"],"metadata":{"id":"IiyeBOFdk8-z"},"id":"IiyeBOFdk8-z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["majority_size = 0.7\n","fraud_rate_disparity = 1\n","typeIII_df = group_prevalence_disparity(large_sample_df,original_sample_df,majority_size,fraud_rate_disparity)\n","typeIII_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Income Data Variants 1m/income_07_type3.csv\")"],"metadata":{"id":"xYbg4CqKlALV"},"id":"xYbg4CqKlALV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["majority_size = 0.7\n","fraud_rate_disparity = 3\n","typeIV_df = group_prevalence_disparity(large_sample_df,original_sample_df,majority_size,fraud_rate_disparity)\n","typeIV_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Income Data Variants 1m/income_07_type4.csv\")"],"metadata":{"id":"04l_tVmfDxLW"},"id":"04l_tVmfDxLW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["majority_size = 0.5\n","fraud_rate_disparity = 1\n","typeV_df = group_prevalence_disparity(large_sample_df,original_sample_df,majority_size,fraud_rate_disparity)\n","typeV_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Income Data Variants 1m/income_07_type5.csv\")"],"metadata":{"id":"IhZj4SuoRSr7"},"id":"IhZj4SuoRSr7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["majority_size = 0.5\n","fraud_rate_disparity = 3\n","typeVI_df = group_prevalence_disparity(large_sample_df,original_sample_df,majority_size,fraud_rate_disparity)\n","typeVI_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/ECE697/Project/Income Data Variants 1m/income_07_type6.csv\")"],"metadata":{"id":"lC2yh7elRVtv"},"id":"lC2yh7elRVtv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZlGHQORcnAtr"},"id":"ZlGHQORcnAtr","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[],"gpuType":"A100"}},"nbformat":4,"nbformat_minor":5}