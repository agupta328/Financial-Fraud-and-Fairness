{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "jLPB82HMw_3s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy_T36bf4BSf",
        "outputId": "857c53ac-14d4-47c4-f2aa-7dde7e075796"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.16)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.6.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uH-aSj9c3l6p",
        "outputId": "c012db82-9bcf-4972-f944-6ec3e3c36b44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aequitas\n",
            "  Using cached aequitas-0.42.0-py3-none-any.whl (2.2 MB)\n",
            "Collecting ohio>=0.2.0 (from aequitas)\n",
            "  Downloading ohio-0.5.0-py3-none-any.whl (26 kB)\n",
            "Collecting Flask==0.12.2 (from aequitas)\n",
            "  Downloading Flask-0.12.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Flask-Bootstrap==3.3.7.1 (from aequitas)\n",
            "  Downloading Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting markdown2==2.3.5 (from aequitas)\n",
            "  Downloading markdown2-2.3.5.zip (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from aequitas) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from aequitas) (1.5.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from aequitas) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.0.16)\n",
            "Collecting tabulate==0.8.2 (from aequitas)\n",
            "  Downloading tabulate-0.8.2.tar.gz (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xhtml2pdf==0.2.2 (from aequitas)\n",
            "  Downloading xhtml2pdf-0.2.2.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from aequitas) (0.12.2)\n",
            "Collecting altair==4.1.0 (from aequitas)\n",
            "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.8/727.8 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting millify==0.1.1 (from aequitas)\n",
            "  Downloading millify-0.1.1.tar.gz (1.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair==4.1.0->aequitas) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair==4.1.0->aequitas) (3.1.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from altair==4.1.0->aequitas) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair==4.1.0->aequitas) (1.22.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair==4.1.0->aequitas) (0.12.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.10/dist-packages (from Flask==0.12.2->aequitas) (2.3.6)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask==0.12.2->aequitas) (8.1.3)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.10/dist-packages (from Flask==0.12.2->aequitas) (2.1.2)\n",
            "Collecting dominate (from Flask-Bootstrap==3.3.7.1->aequitas)\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visitor (from Flask-Bootstrap==3.3.7.1->aequitas)\n",
            "  Downloading visitor-0.1.3.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: html5lib>=1.0 in /usr/local/lib/python3.10/dist-packages (from xhtml2pdf==0.2.2->aequitas) (1.1)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from xhtml2pdf==0.2.2->aequitas) (0.21.0)\n",
            "Collecting pyPdf2 (from xhtml2pdf==0.2.2->aequitas)\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from xhtml2pdf==0.2.2->aequitas) (8.4.0)\n",
            "Collecting reportlab>=3.0 (from xhtml2pdf==0.2.2->aequitas)\n",
            "  Downloading reportlab-4.0.4-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from xhtml2pdf==0.2.2->aequitas) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->aequitas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.1->aequitas) (2022.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.1.1->aequitas) (4.6.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.1.1->aequitas) (2.0.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.0->xhtml2pdf==0.2.2->aequitas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair==4.1.0->aequitas) (2.1.3)\n",
            "Collecting Pillow (from xhtml2pdf==0.2.2->aequitas)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4.1.0->aequitas) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4.1.0->aequitas) (0.19.3)\n",
            "Building wheels for collected packages: Flask-Bootstrap, markdown2, millify, tabulate, xhtml2pdf, visitor\n",
            "  Building wheel for Flask-Bootstrap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-Bootstrap: filename=Flask_Bootstrap-3.3.7.1-py3-none-any.whl size=460122 sha256=2b639f6862effa21bc75b6adaa358606a4f4abdea64aad0c9c880702996d0248\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/33/ad/26540e84a28334e5dfeda756df270f95353779f03bc5cf40d4\n",
            "  Building wheel for markdown2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markdown2: filename=markdown2-2.3.5-py3-none-any.whl size=33327 sha256=37d4c239c4ace700a5724852b0205c26de6fd2dff608db8593359d926aec3b17\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/94/a9/15f2d5685178e7a61701b5f852d2230dcd3a420e4316f8c29c\n",
            "  Building wheel for millify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for millify: filename=millify-0.1.1-py3-none-any.whl size=1844 sha256=41a44671666bdabf170793b6015844ea076e7dbfca101097b749569a1704fb3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/8f/53/2759feac2e247ce89c1165c3ff12d484de7714a875ea3464f0\n",
            "  Building wheel for tabulate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabulate: filename=tabulate-0.8.2-py3-none-any.whl size=23530 sha256=1ed5111fddf6f07f1c85030a120f0cda0dee926f75579762fe86a6f671899a1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/eb/ae/5ae59ba76a9b078413b15eac159e56df2a51e4b1c10d44d099\n",
            "  Building wheel for xhtml2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xhtml2pdf: filename=xhtml2pdf-0.2.2-py3-none-any.whl size=234132 sha256=f77ba6f2fe3ef99c4cce81016a673f48ea6e3f2df131f3ae761f30a903d7fff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/12/14/2cb8ea69b717deb63fbbc1f4a3220b702929c2c90d01ab513b\n",
            "  Building wheel for visitor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visitor: filename=visitor-0.1.3-py3-none-any.whl size=3925 sha256=570961424f856e4f96abcc1176b4bef0348a936ab2103d9b573b3603ce70b825\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/31/99/2ec5b4459cac4d801d6201d501a354366d180afc9f8bb2d333\n",
            "Successfully built Flask-Bootstrap markdown2 millify tabulate xhtml2pdf visitor\n",
            "Installing collected packages: visitor, tabulate, ohio, millify, markdown2, pyPdf2, Pillow, dominate, reportlab, Flask, xhtml2pdf, Flask-Bootstrap, altair, aequitas\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.8.10\n",
            "    Uninstalling tabulate-0.8.10:\n",
            "      Successfully uninstalled tabulate-0.8.10\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 2.2.5\n",
            "    Uninstalling Flask-2.2.5:\n",
            "      Successfully uninstalled Flask-2.2.5\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed Flask-0.12.2 Flask-Bootstrap-3.3.7.1 Pillow-9.5.0 aequitas-0.42.0 altair-4.1.0 dominate-2.8.0 markdown2-2.3.5 millify-0.1.1 ohio-0.5.0 pyPdf2-3.0.1 reportlab-4.0.4 tabulate-0.8.2 visitor-0.1.3 xhtml2pdf-0.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting aequitas==0.42.0 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 1))\n",
            "  Downloading aequitas-0.42.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctgan (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 2))\n",
            "  Downloading ctgan-0.7.3-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 3)) (3.3.5)\n",
            "Collecting matplotlib==3.3.4 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.3.4.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.20.2 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 5))\n",
            "  Downloading numpy-1.20.2.zip (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting optuna==2.10.0 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 6))\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.1.4 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 7))\n",
            "  Downloading pandas-1.1.4.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyYAML==5.4.1 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 8))\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==0.24.2 (from -r /content/drive/MyDrive/summer_23/data/requirements.txt (line 9))\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.2.2 Requires-Python >=3.6,<3.9; 0.2.2.dev1 Requires-Python >=3.5,<3.9; 0.2.2.dev2 Requires-Python >=3.6,<3.9; 0.2.2.dev3 Requires-Python >=3.6,<3.9; 0.3.0 Requires-Python >=3.6,<3.9; 0.3.0.dev0 Requires-Python >=3.5,<3.9; 0.3.0.dev1 Requires-Python >=3.6,<3.9; 0.3.1 Requires-Python >=3.6,<3.9; 0.3.1.dev0 Requires-Python >=3.6,<3.9; 0.3.1.dev1 Requires-Python >=3.6,<3.9; 0.3.1.dev2 Requires-Python >=3.6,<3.9; 0.3.2.dev0 Requires-Python >=3.6,<3.9; 0.4.0 Requires-Python >=3.6,<3.9; 0.4.0.dev0 Requires-Python >=3.6,<3.9; 0.4.0.dev1 Requires-Python >=3.6,<3.9; 0.4.1 Requires-Python >=3.6,<3.9; 0.4.1.dev0 Requires-Python >=3.6,<3.9; 0.4.1.dev1 Requires-Python >=3.6,<3.9; 0.4.2 Requires-Python >=3.6,<3.9; 0.4.2.dev0 Requires-Python >=3.6,<3.9; 0.4.3 Requires-Python >=3.6,<3.9; 0.4.3.dev0 Requires-Python >=3.6,<3.9; 0.4.3.dev1 Requires-Python >=3.6,<3.9; 0.4.4.dev0 Requires-Python >=3.6,<3.9; 0.5.0 Requires-Python >=3.6,<3.10; 0.5.0.dev0 Requires-Python >=3.6,<3.9; 0.5.0.dev1 Requires-Python >=3.6,<3.10; 0.5.1 Requires-Python >=3.6,<3.10; 0.5.1.dev0 Requires-Python >=3.6,<3.10; 0.5.1.dev1 Requires-Python >=3.6,<3.10; 0.5.1.dev2 Requires-Python >=3.6,<3.10; 0.5.1.dev3 Requires-Python >=3.6,<3.10; 0.5.2 Requires-Python >=3.6,<3.10; 0.5.2.dev0 Requires-Python >=3.6,<3.10; 0.5.2.dev1 Requires-Python >=3.6,<3.10; 0.5.3.dev0 Requires-Python >=3.6,<3.10; 0.6.0 Requires-Python >=3.6,<3.10; 0.6.0.dev0 Requires-Python >=3.6,<3.10; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.0rc1 Requires-Python >=3.7,<3.10; 1.7.0rc2 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.6.2 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0, 1.8.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.11.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.6.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "#\n",
        "# The copyright of this file belongs to Feedzai. The file cannot be\n",
        "# reproduced in whole or in part, stored in a retrieval system,\n",
        "# transmitted in any form, or by any means electronic, mechanical,\n",
        "# photocopying, or otherwise, without the prior permission of the owner.\n",
        "#\n",
        "# (c) 2022 Feedzai, Strictly Confidential\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install -r /content/drive/MyDrive/summer_23/data/requirements.txt\n",
        "#!pip install optuna==2.10.0\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iXuSqJwgXfT",
        "outputId": "0ea58fce-d4b9-4fde-852e-a847c812a058"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c988a10-a09a-4fe8-a451-e046145ed8be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c988a10-a09a-4fe8-a451-e046145ed8be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving random_search.py to random_search.py\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgbm  # Tested ML method\n",
        "import numpy as np       # Random number generation\n",
        "import seaborn as sns    # Plotting library\n",
        "import pandas as pd      # Read/write data\n",
        "import yaml              # Read hyperparameter space configuration\n",
        "\n",
        "from aequitas.group import Group                # Fairness metrics\n",
        "from matplotlib import pyplot as plt            # Plotting method\n",
        "from sklearn.preprocessing import LabelEncoder  # Categorical encoding for LGBM models\n",
        "from sklearn import metrics                     # ROC metrics\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('/content/drive/MyDrive/summer_23/data/random_search.py','wb').write(src)\n",
        "from random_search import RandomValueTrial, suggest_callable_hyperparams  # Random search wrapper methods\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "fsj7K9BQgXfX",
        "outputId": "1411ea1e-418f-43f2-b239-4ccbc060f3c1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BI9OMIOOgXfY"
      },
      "outputs": [],
      "source": [
        "# Read hyperparameter space for the LGBM Models, expected structure is presented bellow\n",
        "with open(\"/content/drive/MyDrive/summer_23/data/lightgbm_hyperparameter_space.yaml\", \"r\") as file:\n",
        "    hyperparam_space = yaml.load(file, Loader=yaml.FullLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcGCKyNDgXfY",
        "outputId": "030dc129-ed1b-47b5-b385-63f93559e188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LightGBM': {'classpath': 'lightgbm.LGBMClassifier',\n",
              "  'kwargs': {'n_estimators': {'type': 'int',\n",
              "    'range': [20, 10000],\n",
              "    'log': True},\n",
              "   'max_depth': {'type': 'int', 'range': [3, 30]},\n",
              "   'learning_rate': {'type': 'float', 'range': [0.02, 0.1], 'log': True},\n",
              "   'num_leaves': {'type': 'int', 'range': [10, 100], 'log': True},\n",
              "   'boosting_type': ['gbdt', 'goss'],\n",
              "   'min_data_in_leaf': {'type': 'int', 'range': [5, 200], 'log': True},\n",
              "   'max_bin': {'type': 'int', 'range': [100, 500]},\n",
              "   'enable_bundle': [True, False]}}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# The expected structure is the following:\n",
        "hyperparam_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n_oDwnNgXfZ",
        "outputId": "a0a44185-c37b-4f93-e7cb-d633162b9832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classpath': 'lightgbm.LGBMClassifier',\n",
              " 'n_estimators': 263,\n",
              " 'max_depth': 23,\n",
              " 'learning_rate': 0.020003681922217444,\n",
              " 'num_leaves': 19,\n",
              " 'boosting_type': 'gbdt',\n",
              " 'min_data_in_leaf': 9,\n",
              " 'max_bin': 238,\n",
              " 'enable_bundle': False}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Testing a random search suggestion:\n",
        "trial = RandomValueTrial(seed=1)\n",
        "suggest_callable_hyperparams(trial, hyperparam_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9U5vN8MkgXfZ"
      },
      "outputs": [],
      "source": [
        "# Define path to datasets. Replace `base_path` with the appropriate value.\n",
        "base_path = \"/content/drive/MyDrive/summer_23/data/archive/\"\n",
        "\n",
        "datasets_paths = {\n",
        "    \"Base\":    base_path + \"Base.csv\",\n",
        "    \"TypeI\":   base_path + \"Variant I.csv\"}#,\n",
        "#     \"TypeII\":  base_path + \"TypeII.parquet\",\n",
        "#     \"TypeIII\": base_path + \"TypeIII.parquet\",\n",
        "#     \"TypeIV\":  base_path + \"TypeIV.parquet\",\n",
        "#     \"TypeV\":   base_path + \"TypeV.parquet\",\n",
        "\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KauXc79ygXfa"
      },
      "outputs": [],
      "source": [
        "# Read the datasets with pandas.\n",
        "datasets = {key: pd.read_csv(path) for key, path in datasets_paths.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "n15r9QbXgXfd"
      },
      "outputs": [],
      "source": [
        "# Define the label field and categorical columns.\n",
        "label = \"fraud_bool\"\n",
        "\n",
        "categorical_features = [\n",
        "    \"payment_type\",\n",
        "    \"employment_status\",\n",
        "    \"housing_status\",\n",
        "    \"source\",\n",
        "    \"device_os\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Mu-t94VQgXfd"
      },
      "outputs": [],
      "source": [
        "# Create the train and test sets. Shuffle data with `sample` method.\n",
        "# The split was done by month. The first 6 months as the train, the last 2 months as test.\n",
        "train_dfs = {key: df[df[\"month\"]<6].sample(frac=1, replace=False) for key, df in datasets.items()}\n",
        "test_dfs = {key: df[df[\"month\"]>=6].sample(frac=1, replace=False) for key, df in datasets.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kQ_D1_mfgXfe"
      },
      "outputs": [],
      "source": [
        "# Encode the categorical variables in the datasets to integers.\n",
        "# This is expected by LGBM (or columns with the `categorical` data type).\n",
        "\n",
        "for name in datasets.keys():  # For each dataset in the suite\n",
        "    train = train_dfs[name]\n",
        "    test = test_dfs[name]\n",
        "\n",
        "    for feat in categorical_features:\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(train[feat])  # Fit an encoder to the train set.\n",
        "        train[feat] = encoder.transform(train[feat])  # Transform train set.\n",
        "        test[feat] = encoder.transform(test[feat])    # Transform test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv8lAgQcgXfe",
        "outputId": "412f0c01-8a89-4c9e-d44c-4c8413dda173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
            "  _log_warning('Using categorical_feature in Dataset.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=42\n"
          ]
        }
      ],
      "source": [
        "# Cell with train loop.\n",
        "\n",
        "# Define number of trials in Random search.\n",
        "n_trials=10\n",
        "# Random state for sampling seeds.\n",
        "np.random.seed(42)\n",
        "# Seeds for the random search sampling algorithm.\n",
        "seeds = np.random.choice(list(range(1_000_000)), size=n_trials, replace=False)\n",
        "\n",
        "# Variable to store the results.\n",
        "runs = {}\n",
        "\n",
        "for trial in range(n_trials):\n",
        "    seed = seeds[trial]\n",
        "    trial = RandomValueTrial(seed=seed)\n",
        "    # Hyperparameters for the random search trial.\n",
        "    test_hyperparams = suggest_callable_hyperparams(trial, hyperparam_space)\n",
        "    del test_hyperparams[\"classpath\"] # Remove unnecessary key in hyperparaemters.\n",
        "\n",
        "    # Update list of tested hyperparameters.\n",
        "    prev_hyperparams = runs.get(\"hyperparams\", [])\n",
        "    prev_hyperparams.append(test_hyperparams)\n",
        "    runs[\"hyperparams\"] = prev_hyperparams\n",
        "\n",
        "    for dataset_name in datasets.keys():  # Run hyperparameters on all variants of datastes.\n",
        "        model = lgbm.LGBMClassifier(n_jobs=10, **test_hyperparams)  # Instantiate LGBM Model.\n",
        "        X_train = train_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
        "        y_train = train_dfs[dataset_name][\"fraud_bool\"]\n",
        "        X_test = test_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
        "        y_test = test_dfs[dataset_name][\"fraud_bool\"]\n",
        "        # Fit model to training data.\n",
        "        model.fit(X_train, y_train, categorical_feature=categorical_features)\n",
        "        # Obtain predictions in test data.\n",
        "        predictions = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Obtain ROC curve for the predictions.\n",
        "        fprs, tprs, thresholds = metrics.roc_curve(y_test, predictions)\n",
        "        # Obtain threshold and recall. We select 5% FPR as threshold.\n",
        "        threshold = np.min(thresholds[fprs==max(fprs[fprs < 0.05])])\n",
        "        recall = np.max(tprs[fprs==max(fprs[fprs < 0.05])])\n",
        "\n",
        "        # Binarize predictions for Aequitas.\n",
        "        preds_binary = (predictions > threshold).astype(int)\n",
        "\n",
        "        # Create a dataframe with protected group column, predictions and labels.\n",
        "        # Here, we select age>=50 as threshold.\n",
        "        aequitas_df = pd.DataFrame(\n",
        "            {\n",
        "               \"age\": (X_test[\"customer_age\"]>=50).map({True: \"Older\", False: \"Younger\"}),\n",
        "                \"score\": preds_binary,\n",
        "                \"label_value\": y_test.values\n",
        "\n",
        "            }\n",
        "        )\n",
        "\n",
        "        #print(aequitas_df)\n",
        "        # Obtain FPR results for different groups.\n",
        "        g = Group()\n",
        "        aequitas_results = g.get_crosstabs(aequitas_df, score_thresholds=None, attr_cols=[\"age\"])[0]  #,score_col=[\"preds\"], label_col=[\"y\"]\n",
        "\n",
        "        # Store the results for the trained model\n",
        "        results = {}\n",
        "        results[\"recall\"] = recall\n",
        "        results[\"recall Older\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[\"tpr\"]].values[0][0]\n",
        "        results[\"recall Younger\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[\"tpr\"]].values[0][0]\n",
        "        results[\"fpr Older\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[\"fpr\"]].values[0][0]\n",
        "        results[\"fpr Younger\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[\"fpr\"]].values[0][0]\n",
        "\n",
        "        # Store the results in the runs variable\n",
        "        prev_runs = runs.get(dataset_name, [])\n",
        "        prev_runs.append(results)\n",
        "        runs[dataset_name] = prev_runs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6fq_rpkt-qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bY6v-f79gXff"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe with the results for each model in each dataset.\n",
        "rs_results = pd.DataFrame(runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MRyen1JZgXff"
      },
      "outputs": [],
      "source": [
        "# Helper method to obtain the metric values for a given model.\n",
        "def get_results(results, variant, metric):\n",
        "    col = results[variant]\n",
        "    values = []\n",
        "    for idx, val in col.iteritems():\n",
        "        values.append(val[metric])\n",
        "    return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FIw-UsPCgXff"
      },
      "outputs": [],
      "source": [
        "# Obtain the relevant metrics to plots from the dataframe.\n",
        "variants = list(datasets_paths.keys())\n",
        "\n",
        "plot_results = {\"Variant\": [], \"Recall\": [], \"FPR Ratio\": []}\n",
        "\n",
        "for variant in variants:\n",
        "    plot_results[\"Recall\"] += get_results(rs_results, variant, \"recall\")\n",
        "    # Obtain the FPR if both groups.\n",
        "    for fpr_younger, fpr_older in zip(get_results(rs_results, variant, \"fpr Younger\"), get_results(rs_results, variant, \"fpr Older\")):\n",
        "        # Calculate FPR ratio as higher fpr / lower fpr\n",
        "        if fpr_younger > fpr_older:\n",
        "            plot_results[\"FPR Ratio\"] += [fpr_older/fpr_younger]\n",
        "        else:\n",
        "            plot_results[\"FPR Ratio\"] += [fpr_younger/fpr_older]\n",
        "    plot_results[\"Variant\"] += [variant] * len(get_results(rs_results, variant, \"recall\"))\n",
        "\n",
        "# Create a dataframe for easier plots.\n",
        "plot_results = pd.DataFrame(plot_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nR5twJ-XgXff"
      },
      "outputs": [],
      "source": [
        "# Create a plot with the full results of the random search algorithm.\n",
        "sns.set()\n",
        "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\"})\n",
        "\n",
        "sns.jointplot(data=plot_results, x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\")\n",
        "plt.ylim((0,1));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KL6bzpXogXff"
      },
      "outputs": [],
      "source": [
        "# Create the final plot. Highlight the top models:\n",
        "top_n = 5\n",
        "top_models_base = plot_results.loc[plot_results['Variant'] == 'Base'].sort_values('Recall', ascending=False).head(top_n).index.values\n",
        "top_models = copy.deepcopy(top_models_base)\n",
        "for i in range(1, 6):\n",
        "    top_models = np.r_[top_models, top_models_base + (100 * i)]\n",
        "\n",
        "plot_results['index'] = plot_results.index\n",
        "plot_results['is_top'] = plot_results.apply(lambda x: 1 if x['index'] in top_models else 0, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dndAiUYsgXfg"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\", \"grid.alpha\":0.1})\n",
        "DPI = 200\n",
        "plt.rcParams['figure.dpi'] = DPI\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "# LEFT PLOT\n",
        "sns.scatterplot(ax=ax1, data=plot_results.loc[(~plot_results.index.isin(top_models)), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", alpha=0.2)\n",
        "sns.scatterplot(ax=ax1, data=plot_results.loc[plot_results.index.isin(top_models), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", legend=False)\n",
        "ax1.set(\n",
        "    ylim=(0,1)\n",
        ")\n",
        "\n",
        "# RIGHT PLOT\n",
        "sns.scatterplot(ax=ax2, data=plot_results.loc[(~plot_results.index.isin(top_models)) & (plot_results[\"Variant\"].isin([\"Base\", \"Type II\", \"Type V\", \"Type IV\"])), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", alpha=0.2, palette=[sns.color_palette()[0], sns.color_palette()[2], sns.color_palette()[4], sns.color_palette()[5]], legend=False)\n",
        "sns.scatterplot(ax=ax2, data=plot_results.loc[(plot_results.index.isin(top_models)) & (plot_results[\"Variant\"].isin([\"Base\", \"Type II\", \"Type V\", \"Type IV\"])), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", palette=[sns.color_palette()[0], sns.color_palette()[2], sns.color_palette()[4], sns.color_palette()[5]], legend=False)\n",
        "ax2.set(\n",
        "    ylim=(0,0.4),\n",
        "    ylabel=\"\",\n",
        "    xticks=np.arange(0.2, 0.8, 0.1),\n",
        "    yticks=np.arange(0, 0.5, 0.1),\n",
        "    xlim=(0.2, 0.6),\n",
        ")\n",
        "\n",
        "rect = plt.Rectangle((0.2, 0.004), 0.4, 0.396, facecolor=(0.1, 0.1, 0.1, 0.05), edgecolor=\"grey\", linestyle=\"-\")\n",
        "ax1.add_patch(rect)\n",
        "handles, labels = ax1.get_legend_handles_labels()\n",
        "handles = list(handles) + [rect]\n",
        "labels = list(labels) + [\"Plot on the right\"]\n",
        "ax1.legend(handles, labels, title=\"Variant\")\n",
        "\n",
        "sns.move_legend(\n",
        "    ax1,\n",
        "    loc=\"lower center\",\n",
        "    bbox_to_anchor=[1.08, -0.32],\n",
        "    ncol=7\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results"
      ],
      "metadata": {
        "id": "DbQ8V8MUnKx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plo"
      ],
      "metadata": {
        "id": "PxO44JIiCR0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}